# RAG Evaluation Skill
**Version:** 1.0.0  
**Purpose:** Define standardized evaluation framework for RAG pipelines using RAGAS metrics

## When to Use
- Evaluating retrieval quality in RAG systems
- Assessing answer quality and faithfulness
- Benchmarking RAG pipeline iterations
- Production monitoring of RAG performance

## MUST Rules

### Evaluation Metrics (RAGAS Framework)
- MUST evaluate all four core metrics:
  - **Context Precision**: Relevance of retrieved context to question
  - **Context Recall**: Coverage of ground truth in retrieved context
  - **Faithfulness**: Factual consistency with retrieved context
  - **Answer Relevancy**: Relevance of answer to question
- MUST use separate LLM for evaluation (avoid self-evaluation bias)
- MUST NOT use production LLM as judge LLM
- MUST use ground truth dataset for recall evaluation

### Dataset Requirements
- MUST include fields: `question`, `ground_truth`, `answer`, `contexts`
- MUST have minimum 20 test cases for meaningful results
- MUST cover edge cases (ambiguous queries, multi-hop reasoning)
- MUST sanitize test data (no PII, sensitive information)

### Evaluation Pipeline
- MUST run evaluations offline (not in production inference)
- MUST log evaluation results with timestamps
- MUST track metric trends over time
- MUST set threshold alerts (e.g., faithfulness < 0.7)

### Testing Requirements
- MUST use `GenericFakeChatModel` or stubs for evaluation tests
- MUST NOT make real LLM calls in unit tests
- MUST mock RAGAS metrics for deterministic testing

## Workflow

1. **Prepare Evaluation Dataset**
   ```python
   from datasets import Dataset
   from pydantic import BaseModel, Field
   from typing import List
   
   class EvaluationSample(BaseModel):
       question: str = Field(description="Question to evaluate")
       ground_truth: str = Field(description="Expected answer")
       answer: str = Field(description="Generated answer from RAG")
       contexts: List[str] = Field(description="Retrieved contexts")
   
   eval_data = {
       "question": [...],
       "ground_truth": [...],
       "answer": [...],  # Generated by RAG
       "contexts": [...]  # Retrieved contexts
   }
   dataset = Dataset.from_dict(eval_data)
   ```

2. **Configure Judge LLM and Embeddings**
   ```python
   from ragas import evaluate
   from ragas.metrics import (
       context_precision,
       context_recall,
       faithfulness,
       answer_relevancy
   )
   
   # Use different model than production RAG
   judge_llm = ChatOpenAI(model="gpt-4", temperature=0)
   embeddings = OpenAIEmbeddings()
   ```

3. **Run Evaluation**
   ```python
   result = evaluate(
       dataset=dataset,
       metrics=[
           context_precision,
           context_recall,
           faithfulness,
           answer_relevancy
       ],
       llm=judge_llm,
       embeddings=embeddings
   )
   
   print(result)
   # Log to file with timestamp
   ```

4. **Analyze and Alert**
   ```python
   if result["faithfulness"] < 0.7:
       logger.warning(f"Faithfulness below threshold: {result['faithfulness']}")
   ```

5. **Testing with Mocks**
   ```python
   from langchain_core.language_models.fake_chat_models import GenericFakeChatModel
   
   # For unit tests - use fake model
   fake_llm = GenericFakeChatModel(
       messages=iter(["Mocked evaluation result"])
   )
   
   # Mock RAGAS metrics
   def mock_evaluate(dataset, metrics, llm, embeddings):
       return {
           "context_precision": 0.85,
           "context_recall": 0.78,
           "faithfulness": 0.92,
           "answer_relevancy": 0.88
       }
   ```

## Output Checklist
- [ ] Evaluation dataset with 20+ samples prepared
- [ ] All four RAGAS metrics computed
- [ ] Judge LLM different from production LLM
- [ ] Results logged with timestamp
- [ ] Threshold alerts configured
- [ ] Metric trends tracked over time
- [ ] Ground truth dataset validated
- [ ] Edge cases included in test set
- [ ] Unit tests use GenericFakeChatModel or mocks
- [ ] No real LLM calls in automated tests
